{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c82c90d",
   "metadata": {},
   "source": [
    "# Predicciones Meteorol√≥gicas (AEMET) - SPRINT I\n",
    "\n",
    "Parte 1 - Extracci√≥n de Datos\n",
    "\n",
    "Navegar la documentaci√≥n de la API de AEMET y explorar los endpoints\n",
    "\n",
    "Desarrollar un script que extraiga la informaci√≥n hist√≥rica de todas las provincias.\n",
    "\n",
    "Ejecutar el script para extraer los datos de los √∫ltimos dos a√±os y verificar que todo¬†funcione correctamente.\n",
    "\n",
    "En el modelo de datos, cada registro debe tener un timestamp de extracci√≥n y un identificador para que se pueda manejar el sistema de actualizaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = \"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqb3JnZXJpdmVyb2RlbG9zcmlvc0BnbWFpbC5jb20iLCJqdGkiOiJiMjlhZmM2Zi0yMTkwLTQ4ZTEtYjlmYy01NGY5OTk3OTc1YjUiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTc0ODk2ODY4NSwidXNlcklkIjoiYjI5YWZjNmYtMjE5MC00OGUxLWI5ZmMtNTRmOTk5Nzk3NWI1Iiwicm9sZSI6IiJ9.90idEjGLaI61xKuPe8sdQtBJ2fdf4gwZmsww11V1VpE\"\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"‚ùå No se encontr√≥ AEMET_API_KEY en las variables de entorno.\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Funci√≥n para obtener el inventario completo de estaciones\n",
    "# ---------------------------------------------------\n",
    "def obtener_inventario_completo():\n",
    "    url_inventario = (\n",
    "        \"https://opendata.aemet.es/opendata/api/\"\n",
    "        \"valores/climatologicos/inventarioestaciones/todasestaciones\"\n",
    "    )\n",
    "    r = requests.get(url_inventario, params={\"api_key\": API_KEY}, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    datos_meta = r.json().get(\"datos\")\n",
    "    if not datos_meta:\n",
    "        raise RuntimeError(\"No se obtuvo URL de datos del inventario.\")\n",
    "    r2 = requests.get(datos_meta, timeout=15)\n",
    "    r2.raise_for_status()\n",
    "    estaciones = r2.json()  # Lista de diccionarios\n",
    "    return pd.DataFrame(estaciones)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Funci√≥n para descargar datos diarios para una estaci√≥n\n",
    "# ---------------------------------------------------\n",
    "def descargar_para_una_estacion(idema: str, nombre: str, id_descarga: str, bloques_fechas) -> list:\n",
    "    filas = []\n",
    "    for fecha_ini, fecha_fin in bloques_fechas:\n",
    "        url_meta = (\n",
    "            \"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/\"\n",
    "            f\"datos/fechaini/{fecha_ini}/fechafin/{fecha_fin}/estacion/{idema}\"\n",
    "        )\n",
    "        try:\n",
    "            r = requests.get(url_meta, params={\"api_key\": API_KEY}, timeout=15)\n",
    "            if r.status_code != 200:\n",
    "                continue\n",
    "            datos_meta = r.json()\n",
    "            url_real  = datos_meta.get(\"datos\")\n",
    "            if not url_real:\n",
    "                continue\n",
    "\n",
    "            rd = requests.get(url_real, timeout=15)\n",
    "            if rd.status_code != 200:\n",
    "                continue\n",
    "\n",
    "            lista_json = rd.json()\n",
    "            for rec in lista_json:\n",
    "                rec[\"idema\"]               = idema\n",
    "                rec[\"nombre_estacion\"]     = nombre\n",
    "                rec[\"timestamp_extraccion\"]= datetime.utcnow().isoformat()\n",
    "                rec[\"id_descarga\"]         = id_descarga\n",
    "                filas.append(rec)\n",
    "\n",
    "            time.sleep(1.2)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return filas\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Script principal\n",
    "# ---------------------------------------------------\n",
    "def main():\n",
    "    print(\"üì• Iniciando extracci√≥n de datos de TODAS las estaciones...\")\n",
    "\n",
    "    # 1) Obtengo el inventario completo (todas las estaciones)\n",
    "    estaciones_df = obtener_inventario_completo()\n",
    "    estaciones_df = estaciones_df.dropna(subset=[\"indicativo\"])  # descartar filas sin ID v√°lido\n",
    "\n",
    "    # 2) Defino rangos de fechas autom√°ticos de √∫ltimos 2 a√±os, en 4 bloques de ~6 meses\n",
    "    hoy = datetime.utcnow().date()\n",
    "    hace_dos_a√±os = hoy - timedelta(days=730)\n",
    "    bloques = []\n",
    "    inicio = hace_dos_a√±os\n",
    "    while inicio < hoy:\n",
    "        fin = inicio + timedelta(days=182)\n",
    "        if fin > hoy:\n",
    "            fin = hoy\n",
    "        bloques.append((f\"{inicio.isoformat()}T00:00:00UTC\", f\"{fin.isoformat()}T00:00:00UTC\"))\n",
    "        inicio = fin + timedelta(days=1)\n",
    "\n",
    "    # 3) Veo si ya hay un CSV de salida previo, para no re-descargar estaciones\n",
    "    ARCHIVO_SALIDA = \"data/temperaturas_historicas_todas.csv\"\n",
    "    ya_descargadas = set()\n",
    "    if os.path.exists(ARCHIVO_SALIDA):\n",
    "        try:\n",
    "            df_prev = pd.read_csv(ARCHIVO_SALIDA, dtype=str, usecols=[\"idema\"])\n",
    "            ya_descargadas = set(df_prev[\"idema\"].dropna().unique())\n",
    "        except Exception:\n",
    "            ya_descargadas = set()\n",
    "\n",
    "    # 4) Genero un UUID para esta ejecuci√≥n\n",
    "    id_descarga = str(uuid.uuid4())\n",
    "    print(f\"   ‚Ä¢ UUID de esta descarga: {id_descarga}\")\n",
    "    print(f\"   ‚Ä¢ Total de estaciones a procesar: {len(estaciones_df)}\")\n",
    "\n",
    "    # 5) Recorro cada estaci√≥n\n",
    "    todas_las_filas = []\n",
    "    for idx, fila in estaciones_df.iterrows():\n",
    "        idema  = fila[\"indicativo\"]\n",
    "        nombre = fila.get(\"nombre\", \"\")\n",
    "\n",
    "        if idema in ya_descargadas:\n",
    "            print(f\"‚ûñ Saltando {idema} (ya descargada antes)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üì° Procesando estaci√≥n: {idema} ‚Äî {nombre} ({idx+1}/{len(estaciones_df)})\")\n",
    "        filas_est = descargar_para_una_estacion(idema, nombre, id_descarga, bloques)\n",
    "        todas_las_filas.extend(filas_est)\n",
    "\n",
    "    # 6) Guardo todo en un CSV final\n",
    "    if todas_las_filas:\n",
    "        df_final = pd.DataFrame(todas_las_filas)\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        if os.path.exists(ARCHIVO_SALIDA):\n",
    "            df_final.to_csv(ARCHIVO_SALIDA, mode=\"a\", index=False, header=False)\n",
    "        else:\n",
    "            df_final.to_csv(ARCHIVO_SALIDA, index=False)\n",
    "        print(f\"‚úÖ Extracci√≥n completada. Guardado: '{ARCHIVO_SALIDA}'\")\n",
    "        print(f\"   ‚Üí Filas nuevas: {len(df_final)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No se obtuvieron datos nuevos en esta ejecuci√≥n.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef11a6-2631-461d-a84a-67ca5f595c4b",
   "metadata": {},
   "source": [
    "Parte 2 - Limpieza de Datos\n",
    "\n",
    "Hacer limpieza general de datos\n",
    "\n",
    "Modelar los datos para trabajar c√≥modamente en una base de datos\n",
    "\n",
    "Ejecutar los scripts de recopilaci√≥n de datos\n",
    "\n",
    "Considerar aplicar transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae801f-7b06-41d0-9e95-2b4d6b018cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Cargar CSV \"completo\"\n",
    "df = pd.read_csv(\"data/temperaturas_historicas_todas.csv\", dtype=str)\n",
    "\n",
    "# 2) Convertir columns num√©ricas y fechas\n",
    "num_cols = [\"tmin\",\"tmax\",\"tmed\",\"prec\",\"velmedia\",\"racha\",\"hrMedia\",\"altitud\"]\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col].astype(str).str.replace(\",\", \".\", regex=False), errors=\"coerce\")\n",
    "\n",
    "df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "df[\"timestamp_extraccion\"] = pd.to_datetime(df[\"timestamp_extraccion\"], errors=\"coerce\")\n",
    "\n",
    "# 3) Seleccionar solo las columnas de inter√©s\n",
    "df = df[[\n",
    "    \"id_descarga\",\n",
    "    \"indicativo\",      # Si quieres renombrar, luego veremos\n",
    "    \"nombre\",\n",
    "    \"provincia\",\n",
    "    \"altitud\",\n",
    "    \"fecha\",\n",
    "    \"tmin\",\"tmax\",\"tmed\",\"prec\",\"velmedia\",\"racha\",\"hrMedia\",\n",
    "    \"timestamp_extraccion\"\n",
    "]]\n",
    "\n",
    "# 4) Ordenar por estaci√≥n y fecha\n",
    "df = df.sort_values([\"indicativo\", \"fecha\"]).reset_index(drop=True)\n",
    "\n",
    "# 5) Imputaci√≥n de nulos (gap_threshold=3)\n",
    "def fill_station_gaps(grp):\n",
    "    grp = grp.copy()\n",
    "    # Definir periodo activo de esta estaci√≥n\n",
    "    start, end = grp[\"fecha\"].min(), grp[\"fecha\"].max()\n",
    "    grp = grp[(grp[\"fecha\"] >= start) & (grp[\"fecha\"] <= end)]\n",
    "    \n",
    "    for col in [\"tmin\",\"tmax\",\"tmed\",\"prec\",\"velmedia\",\"racha\",\"hrMedia\"]:\n",
    "        serie = grp[col]\n",
    "        mediana = serie.median()\n",
    "        is_nan = serie.isna()\n",
    "        grupos = (is_nan != is_nan.shift()).cumsum()\n",
    "        \n",
    "        # Rellenar huecos peque√±os con mediana\n",
    "        for g in grupos[is_nan].unique():\n",
    "            idx_gap = grupos[grupos == g].index\n",
    "            if len(idx_gap) <= 3:\n",
    "                grp.loc[idx_gap, col] = mediana\n",
    "                \n",
    "        # Interpolar lineal en huecos mayores\n",
    "        grp[col] = grp[col].interpolate(method=\"linear\", limit_direction=\"both\")\n",
    "        # Si quedan nulos, rellenar con mediana\n",
    "        grp[col] = grp[col].fillna(mediana)\n",
    "    \n",
    "    return grp\n",
    "\n",
    "df = df.groupby(\"indicativo\").apply(fill_station_gaps)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 6) Eliminar filas que a√∫n tengan tmed = NaN (nuestra variable target)\n",
    "df = df.dropna(subset=[\"tmed\"]).reset_index(drop=True)\n",
    "\n",
    "# 7) Rellenar precipitaci√≥n faltante con 0\n",
    "df[\"prec\"] = df[\"prec\"].fillna(0)\n",
    "\n",
    "# 8) A√±adir id √∫nico de limpieza por fila\n",
    "df[\"id_limpieza\"] = range(len(df))\n",
    "\n",
    "# 9) Guardar resultado limpio\n",
    "df.to_csv(\"data/temperaturas_limpias.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Limpieza terminada. Dimensiones finales:\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
